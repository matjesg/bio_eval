{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook\n",
    "from outliers import smirnov_grubbs as grubbs\n",
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%cd /media/data/DeepFLaSH/_bio_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Choose Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Lab Inns_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'inns_01'\n",
    "name = 'Lucas'\n",
    "groups = {'Ctrl' : 0, 'Ext' : 1}\n",
    "MASK = 'cFOS'\n",
    "OUTLIERS = ['1080']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Lab Inns_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'inns_02'\n",
    "name = 'Anupam'\n",
    "groups = {'Saline' : 0, 'L-DOPA responder' : 1, 'L-DOPA non-responder' : 2}\n",
    "MASK = 'cFOS'\n",
    "OUTLIERS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Lab Mue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'mue'\n",
    "name = 'Maren'\n",
    "groups = {'Ext' : 0, 'Ret' : 1}\n",
    "MASK = 'cFOS'\n",
    "OUTLIERS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Lab Wue_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'wue_02'\n",
    "name = 'Lab_Wue_02'\n",
    "groups = {'WT' : 0, 'KO' : 1}\n",
    "MASK = 'Parv'\n",
    "OUTLIERS = ['1699']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Run Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "df_zuordnung = pd.read_excel('../data/Zuordnung_corr.xlsx')\n",
    "df_zu = df_zuordnung[(df_zuordnung['name']==name)&\n",
    "                (df_zuordnung['Experiment']=='Test')&\n",
    "                (df_zuordnung['Cross-coder Training'].isna()) & \n",
    "                (df_zuordnung['Ausschluss von Analyse'].isna()) &\n",
    "                (df_zuordnung['broken'].isna())].copy()\n",
    "df_zu['Group'] = df_zu['Kondition'].transform(lambda x: groups[x])\n",
    "#df_cfos = pd.read_csv(os.path.join(LAB,LAB+'_' + MASK + '_results.csv'))\n",
    "df_cfos = pd.read_csv(os.path.join(LAB,LAB+'_' + MASK + '_ROIs.csv'))\n",
    "df_neun = pd.read_csv(os.path.join(LAB,LAB+'_NeuN_results.csv'))\n",
    "\n",
    "#Merge Data\n",
    "df = df_cfos.groupby(['ens', 'unet', 'fold', 'Nummer']).mean_intensity.agg(['mean', 'count'])\n",
    "df = df.join(df_neun.set_index('Nummer')[['Neun_Area']], how='left', on='Nummer')\n",
    "df = df.join(df_zu.set_index('Nummer')[['Kondition', 'Group']], how='left', on='Nummer')\n",
    "\n",
    "#Normalize counts\n",
    "df['cfos_per_area'] = df['count']/df['Neun_Area']\n",
    "ctlr_norm = df[df.Group == 0].groupby(['ens', 'unet', 'fold'])[['cfos_per_area', 'mean']].agg(np.mean)\n",
    "df = df.join(ctlr_norm, rsuffix='_ctrl')\n",
    "df['norm_cfos_per_area'] = df['cfos_per_area']/df['cfos_per_area_ctrl']\n",
    "df['norm_mean_intensity'] = df['mean']/df['mean_ctrl']\n",
    "\n",
    "#Remove Outliers\n",
    "df = df[~df.index.isin(OUTLIERS, level=3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df.groupby('Nummer')[['norm_mean_intensity', 'norm_cfos_per_area']].agg('count').sort_values(by='norm_mean_intensity').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_outlier_names(x):\n",
    "    out_ind = grubbs.two_sided_test_indices(x, alpha=0.05)\n",
    "    if len(out_ind)>0:\n",
    "        return str(x.iloc[out_ind].index.get_level_values('Nummer').values)\n",
    "    else:\n",
    "        return 'No Outliers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#df = df[df.index.get_level_values('unet').str[:3]=='ens']\n",
    "df_out = df.groupby(['ens', 'unet', 'fold', 'Kondition'])[['norm_cfos_per_area', 'norm_mean_intensity']].agg(get_outlier_names).reset_index()\n",
    "ax = sns.countplot(x=\"Kondition\", hue='norm_cfos_per_area', data=df_out).set_title(LAB)\n",
    "plt.show()\n",
    "ax = sns.countplot(x=\"Kondition\", hue='norm_mean_intensity', data=df_out).set_title(LAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Compute effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "c_alpha = 0.05\n",
    "for key2, grp2 in tqdm_notebook(df.groupby(['ens', 'unet', 'fold'])):\n",
    "\n",
    "    for m in ['norm_cfos_per_area', 'norm_mean_intensity']:\n",
    "        res_dict = {'ens': [key2[0]], 'unet': [key2[1]],\n",
    "                    'fold': [key2[2]], 'type': [m]}\n",
    "        \n",
    "        df_fil = grp2[~grp2[m].isna()]\n",
    "        \n",
    "        if all(df_fil.groupby('Group').size()>3):\n",
    "            grp_data = [x[1] for x in df_fil.groupby('Group', sort=True)[m]]\n",
    "            n_groups = len(grp_data)\n",
    "            \n",
    "            # Get Group Means/Median\n",
    "            grp_means = [x.mean() for x in grp_data]   \n",
    "            grp_medians = [x.median() for x in grp_data]       \n",
    "            \n",
    "            # Perform the Shapiro-Wilk test for normality.\n",
    "            grp_norms = [x[1] for x in map(stats.shapiro, grp_data)]\n",
    "            \n",
    "            #Check for equality of variances\n",
    "            if len(grp_data)>2:\n",
    "                _, levene = stats.levene(*grp_data, center = 'mean')\n",
    "            else:\n",
    "                _, levene = stats.levene(*grp_data, center = 'median')\n",
    "            \n",
    "            # Anova possible?\n",
    "            anova = True if all(np.array([*grp_norms,levene])> 0.05) else False\n",
    "            res_dict['anova_ok'] = [anova]\n",
    "        \n",
    "            # Only 3 groups tests\n",
    "            if n_groups==3: \n",
    "                \n",
    "                # kruskal all groups\n",
    "                H, kwa_p_value = stats.kruskal(*grp_data)\n",
    "                N = len(df_fil)\n",
    "                \n",
    "                kwa_eta_squared = (H - n_groups + 1)/(N-n_groups) # http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf\n",
    "                # kwa_eta_squared = ((H / (n_groups-1)) * (n_groups-1)) / ((H / (n_groups-1)) * (n_groups-1) + (N-n_groups)) \n",
    "                #For references, see: https://www.researchgate.net/post/Anyone_know_how_to_calculate_eta_squared_for_a_Kruskal-Wallis_analysis\n",
    "                res_dict['eta^2_kwa_all'] = [kwa_eta_squared]\n",
    "                res_dict['p_kwa_all'] = [kwa_p_value]\n",
    "                res_dict['kwa_all'] = 1 if kwa_p_value<0.05 else 0\n",
    "                # Critical Value\n",
    "                p = 1-c_alpha\n",
    "                ddof = n_groups-1\n",
    "                H_c = stats.chi2.ppf(p, ddof) \n",
    "                kwa_eta_squared_c = (H_c- n_groups + 1)/(N-n_groups) \n",
    "                res_dict['c_kwa_all'] = [kwa_eta_squared_c]\n",
    "                \n",
    "                # ANOVA All groups\n",
    "                aov = pg.anova(dv=m, between='Group', data=df_fil, detailed=False)\n",
    "                aov_p_value = aov['p-unc'][0]\n",
    "                res_dict['eta^2_aov_all'] = [aov['np2'][0]]\n",
    "                res_dict['p_aov_all'] = [aov_p_value]\n",
    "                res_dict['aov_all'] = 1 if aov_p_value<0.05 else 0\n",
    "                # Critical Value\n",
    "                # Calculating partial eta-square (fval * ddof1) / (fval * ddof1 + ddof2) \n",
    "                # https://pingouin-stats.org/_modules/pingouin/parametric.html#anova\n",
    "                ddof1 = n_groups - 1\n",
    "                ddof2 = N - n_groups\n",
    "                fval = stats.f.ppf(p, ddof1, ddof2) \n",
    "                aov_eta_squared_c = (fval * ddof1) / (fval * ddof1 + ddof2) \n",
    "                res_dict['c_aov_all'] = [aov_eta_squared_c]\n",
    "\n",
    "            # Pairwise tests\n",
    "            for i,j in itertools.combinations(range(len(grp_data)), 2):\n",
    "                k, l = i+1, j+1\n",
    "                \n",
    "                # Pairwise mannwhitneyu tests\n",
    "                U, mwu_p_value = stats.mannwhitneyu(grp_data[i],grp_data[j], alternative = 'two-sided')\n",
    "                n_1 = grp_data[i].count()\n",
    "                n_2 = grp_data[j].count()\n",
    "                N = n_1 + n_2\n",
    "                mwu_eta_squared = ((U - (n_1*n_2/2)) / np.sqrt((n_1*n_2*(n_1+n_2+1))/12) / np.sqrt(n_1+n_2))**2\n",
    "                # according to http://www.statisticslectures.com/topics/mannwhitneyu/ & cross-checked with Origin & psychometrica\n",
    "                res_dict['eta^2_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared]\n",
    "                res_dict['p_mwu{}_vs_{}'.format(k,l)] = [mwu_p_value]\n",
    "                # Critical Value\n",
    "                p = 1-c_alpha if n_groups==2 else 1-(c_alpha/n_groups)\n",
    "                U_c = stats.chi2.ppf(p, 1)\n",
    "                mwu_eta_squared_c = U_c/N \n",
    "                # Eta squared can be calculated as η²=r²=chi²/N. Note that the Kruskal-Wallis H test statistic is approximately chi²-distributed.\n",
    "                res_dict['c_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared_c]\n",
    "                # Direction check  \n",
    "                mwu_i_vs_j = 0  \n",
    "                if grp_medians[i] != grp_medians[j]:\n",
    "                    if all((n_groups==2, mwu_p_value <= c_alpha)) ^ all((n_groups>2, mwu_p_value <= c_alpha/n_groups)):\n",
    "                        mwu_i_vs_j = k if (grp_medians[i] > grp_medians[j]) else l\n",
    "                res_dict['mwu{}_vs_{}'.format(k,l)] = [mwu_i_vs_j] \n",
    "                \n",
    "                # Pairwise ANOVA\n",
    "                aov = pg.anova(dv=m, between='Group', data=df_fil[df_fil['Group'].isin([i,j])], detailed=False)\n",
    "                aov_p_value = aov['p-unc'][0]   \n",
    "                res_dict['eta^2_aov{}_vs_{}'.format(k,l)] = [aov['np2'][0]]\n",
    "                res_dict['p_aov{}_vs_{}'.format(k,l)] = [aov_p_value]\n",
    "                # Critical Value\n",
    "                ddof1 = 2-1\n",
    "                ddof2 = N-2\n",
    "                fval = stats.f.ppf(p, ddof1, ddof2) \n",
    "                aov_eta_squared_c = (fval * ddof1) / (fval * ddof1 + ddof2) \n",
    "                res_dict['c_aov{}_vs_{}'.format(k,l)] = [aov_eta_squared_c]\n",
    "                # Direction check\n",
    "                aov_i_vs_j = 0  \n",
    "                if grp_means[i] != grp_means[j]:\n",
    "                    if all((n_groups==2, aov_p_value <= c_alpha)) ^ all((n_groups>2, aov_p_value <= c_alpha/n_groups)):\n",
    "                        aov_i_vs_j = k if (grp_means[i] > grp_means[j]) else l\n",
    "                res_dict['aov{}_vs_{}'.format(k,l)] = [aov_i_vs_j] \n",
    "                \n",
    "                 \n",
    "        df_list += [pd.DataFrame(res_dict)]\n",
    "\n",
    "df_final = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_final.to_csv(os.path.join('_results/'+ LAB +'_effect.csv'), index=False)\n",
    "df_final[df_final.type=='norm_cfos_per_area'].to_csv(os.path.join('_results/'+ LAB+'_effect_count.csv'), index=False)\n",
    "df_final[df_final.type=='norm_mean_intensity'].to_csv(os.path.join('_results/'+ LAB+'_effect_intensity.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
