{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy import stats\n",
    "from outliers import smirnov_grubbs as grubbs\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "if ROOT_DIR.endswith(\"notebooks\"):\n",
    "    # Go up one level to the repo root\n",
    "    os.chdir(os.path.dirname(ROOT_DIR))\n",
    "    \n",
    "# Import unet library\n",
    "from unet import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Quantification and statistics for Figure 5, S9, S10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "__lab-inns1__ (Figure S9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'inns1'\n",
    "MASK = 'cFOS'\n",
    "MIN_PIXEL = 16\n",
    "GROUPS = {'Ctrl' : 0, 'Ext' : 1}\n",
    "OUTLIERS = ['1080']\n",
    "FIG = 'S9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "__lab-inns2__ (Figure 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'inns2'\n",
    "MASK = 'cFOS'\n",
    "MIN_PIXEL = 60\n",
    "GROUPS = {'Saline' : 0, 'L-DOPA responder' : 1, 'L-DOPA non-responder' : 2}\n",
    "OUTLIERS = []\n",
    "FIG = 'S10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "__lab-mue__ (Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'mue'\n",
    "MIN_PIXEL = 30\n",
    "MASK = 'cFOS'\n",
    "GROUPS = {'Ext' : 0, 'Ret' : 1}\n",
    "OUTLIERS = []\n",
    "FIG = '5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "__lab-wue2__ (Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'wue2'\n",
    "MIN_PIXEL = 112\n",
    "MASK = 'Parv'\n",
    "GROUPS = {'WT' : 0, 'KO' : 1}\n",
    "MASK = 'Parv'\n",
    "OUTLIERS = ['1699']\n",
    "FIG = '5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Measure ROIs and area for all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Set paths and get model names\n",
    "data_path = 'bioimage_data/lab-{}/images'.format(LAB)\n",
    "mask_path = 'bioimage_data/lab-{}/labels'.format(LAB)\n",
    "area_path = 'bioimage_data/lab-{}/regions'.format(LAB)\n",
    "init_list = [x for x in next(os.walk(mask_path))[1] if not x.startswith('.')]\n",
    "ens_list = [x for x in next(os.walk(os.path.join(mask_path,init_list[0])))[1] if not x.startswith('.')]\n",
    "\n",
    "# Load images and areas\n",
    "file_ids = [each.rsplit('.',1)[0] for each in os.listdir(data_path) if each.endswith('.tif')]\n",
    "image_list = [io.imread(os.path.join(data_path, img_name), as_gray=True) for \n",
    "              img_name in [s + '.tif' for s in file_ids]]\n",
    "area_list = [io.imread(os.path.join(area_path, img_name), as_gray=True) for \n",
    "              img_name in [s + '.tif' for s in file_ids]]\n",
    "\n",
    "# Load and measure area\n",
    "df_area_list = [utils.measure_rois(a, img, fid) for a, img, fid in zip(area_list, image_list, file_ids)]\n",
    "df_area = pd.concat(df_area_list)\n",
    "df_area.rename({'area': 'region_area'}, axis='columns', inplace=True)\n",
    "\n",
    "# Loop over models\n",
    "df_roi_list = []\n",
    "for init in tqdm_notebook(init_list):\n",
    "    for ens in tqdm_notebook(ens_list):\n",
    "        fold_list = [x for x in next(os.walk(os.path.join(mask_path, init, ens)))[1] if not x.startswith('.')]\n",
    "        model_list = [[x for x in next(os.walk(os.path.join(mask_path, init, ens, fold)))[1] if not x.startswith('.')][0] for fold in fold_list]\n",
    "        \n",
    "        for fold, model in zip(fold_list, model_list):\n",
    "            model_path = os.path.join(mask_path, init, ens, fold, model)\n",
    "            \n",
    "            # Load masks\n",
    "            mask_list = [io.imread(os.path.join(model_path, x), as_gray=True).astype('int') for x in [s + '_' + MASK + '.png' for s in file_ids]]\n",
    "            \n",
    "            # Cut irrelevant regions\n",
    "            for msk, area in zip(mask_list, area_list):\n",
    "                msk[area==0]=0\n",
    "                \n",
    "            # Measure ROIs\n",
    "            df_list = [utils.measure_rois(msk, img, fid, min_pixel=MIN_PIXEL) for msk, img, fid in zip(mask_list, image_list, file_ids)]\n",
    "            \n",
    "            # Create output df\n",
    "            df_tmp = pd.concat(df_list)\n",
    "            df_tmp['init'] = init\n",
    "            df_tmp['model'] = fold\n",
    "            df_tmp['unet'] = model\n",
    "            df_tmp['ens'] = ens\n",
    "            df_roi_list += [df_tmp]\n",
    "    \n",
    "    # Concat dataframe\n",
    "    df_roi = pd.concat(df_roi_list)\n",
    "    df_roi = pd.merge(df_roi, df_area[['ID','region_area']], how='left', on='ID')\n",
    "\n",
    "# Save results\n",
    "df_roi.to_csv(os.path.join('source_data', 'fig_'+FIG, 'ROIs_'+LAB+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Combine results with experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get experimental image mapping\n",
    "assignment = pd.read_excel('bioimage_data/image_mapping.xlsx')\n",
    "df_zu = assignment[(assignment['name']=='lab-'+LAB)&\n",
    "                    (assignment['experiment']=='Test')&\n",
    "                    (assignment['training'].isna())].copy()\n",
    "df_zu['Group'] = df_zu['condition'].transform(lambda x: GROUPS[x])\n",
    "\n",
    "# Merge data\n",
    "df_roi = pd.read_csv(os.path.join('source_data', 'fig_'+FIG, 'ROIs_'+LAB+'.csv'))\n",
    "df = df_roi.groupby(['ens', 'unet', 'init', 'model', 'ID', 'region_area']).mean_intensity.agg(['mean', 'count'])\n",
    "df = df.join(df_zu.set_index('ID')[['condition', 'Group', 'area']], how='left', on='ID')\n",
    "df = df.reset_index(level=['region_area'])\n",
    "\n",
    "# Normalize data\n",
    "df['cfos_per_area'] = df['count']/df['region_area']\n",
    "ctlr_norm = df[df.Group == 0].groupby(['ens', 'unet', 'init', 'model'])[['cfos_per_area', 'mean']].agg(np.mean)\n",
    "df = df.join(ctlr_norm, rsuffix='_ctrl')\n",
    "df['norm_cfos_per_area'] = df['cfos_per_area']/df['cfos_per_area_ctrl']\n",
    "df['norm_mean_intensity'] = df['mean']/df['mean_ctrl']\n",
    "\n",
    "# Save to Figure source data\n",
    "df.to_csv('source_data/fig_{}/quantifications_{}.csv'.format(FIG, LAB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Outlier check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_outlier_names(x):\n",
    "    out_ind = grubbs.two_sided_test_indices(x, alpha=0.05)\n",
    "    if len(out_ind)>0:\n",
    "        return str(x.iloc[out_ind].index.get_level_values('ID').values)\n",
    "    else:\n",
    "        return 'No Outliers'\n",
    "\n",
    "# Find outliers in the most trustworthy models (consensus ensembles)\n",
    "df_out = df.reset_index(level=['model', 'ens'])\n",
    "df_out = (df_out[(df_out.model=='ensemble')&(df_out.ens.str.startswith('consensus'))]\n",
    "          .groupby(['ens', 'unet', 'init', 'model', 'condition'])[['norm_cfos_per_area', 'norm_mean_intensity']]\n",
    "          .agg(get_outlier_names).reset_index())\n",
    "ax = sns.countplot(x=\"condition\", hue='norm_cfos_per_area', data=df_out).set_title(LAB)\n",
    "plt.show()\n",
    "ax = sns.countplot(x=\"condition\", hue='norm_mean_intensity', data=df_out).set_title(LAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "df = df.reset_index(level=['ID'])\n",
    "df = df[~df['ID'].isin(OUTLIERS)]\n",
    "\n",
    "df_list = []\n",
    "c_alpha = 0.05\n",
    "c_alpha_list = [0.01, 0.001]\n",
    "for key2, grp2 in tqdm_notebook(df.groupby(['ens', 'unet', 'model', 'area', 'init'])):\n",
    "\n",
    "    for m in ['norm_cfos_per_area']:\n",
    "        res_dict = {'subarea': [key2[3]], 'ens': [key2[0]], 'unet': [key2[1]],\n",
    "                    'model': [key2[2]], 'init': [key2[4]], 'type': [m]}\n",
    "        \n",
    "        df_fil = grp2[~grp2[m].isna()]\n",
    "        \n",
    "        if all(df_fil.groupby('Group').size()>3):\n",
    "            grp_data = [x[1] for x in df_fil.groupby('Group', sort=True)[m]]\n",
    "            n_groups = len(grp_data)\n",
    "            \n",
    "            # Get Group Means\n",
    "            grp_means = [x.mean() for x in grp_data]  \n",
    "            for i, mean in enumerate(grp_means):\n",
    "                res_dict['grp_mean_{}'.format(i+1)] = mean\n",
    "                \n",
    "            # Get Group Medians\n",
    "            grp_medians = [x.median() for x in grp_data]  \n",
    "            for i, med in enumerate(grp_medians):\n",
    "                res_dict['grp_median_{}'.format(i+1)] = med\n",
    "                \n",
    "            # Get Group SD\n",
    "            grp_std = [x.std() for x in grp_data]  \n",
    "            for i, std in enumerate(grp_std):\n",
    "                res_dict['grp_std_{}'.format(i+1)] = std\n",
    "            \n",
    "            # Perform the Shapiro-Wilk test for normality.\n",
    "            grp_norms = [x[1] for x in map(stats.shapiro, grp_data)]\n",
    "            \n",
    "            #Check for equality of variances\n",
    "            if len(grp_data)>2:\n",
    "                _, levene = stats.levene(*grp_data, center = 'mean')\n",
    "            else:\n",
    "                _, levene = stats.levene(*grp_data, center = 'median')\n",
    "            \n",
    "            # Anova possible?\n",
    "            anova = True if all(np.array([*grp_norms,levene])> 0.05) else False\n",
    "            res_dict['anova_ok'] = [anova]\n",
    "        \n",
    "            # Only 3 groups tests\n",
    "            if n_groups==3: \n",
    "                \n",
    "                # kruskal-wallis anova all groups\n",
    "                H, kwa_p_value = stats.kruskal(*grp_data)\n",
    "                N = len(df_fil)\n",
    "                \n",
    "                kwa_eta_squared = (H - n_groups + 1)/(N-n_groups) # http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf\n",
    "                # kwa_eta_squared = ((H / (n_groups-1)) * (n_groups-1)) / ((H / (n_groups-1)) * (n_groups-1) + (N-n_groups)) \n",
    "                #For references, see: https://www.researchgate.net/post/Anyone_know_how_to_calculate_eta_squared_for_a_Kruskal-Wallis_analysis\n",
    "                res_dict['U_kwa_all'] = [H]\n",
    "                res_dict['eta^2_kwa_all'] = [kwa_eta_squared]\n",
    "                res_dict['p_kwa_all'] = [kwa_p_value]\n",
    "                res_dict['kwa_all'] = 1 if kwa_p_value<0.05 else 0\n",
    "                # Critical Value\n",
    "                p = 1-c_alpha\n",
    "                ddof = n_groups-1\n",
    "                H_c = stats.chi2.ppf(p, ddof) \n",
    "                kwa_eta_squared_c = (H_c- n_groups + 1)/(N-n_groups) \n",
    "                res_dict['c_kwa_all'] = [kwa_eta_squared_c]\n",
    "\n",
    "            # Pairwise tests\n",
    "            for i,j in itertools.combinations(range(len(grp_data)), 2):\n",
    "                k, l = i+1, j+1\n",
    "                \n",
    "                # Pairwise mannwhitneyu tests\n",
    "                U, mwu_p_value = stats.mannwhitneyu(grp_data[i],grp_data[j], alternative = 'two-sided')\n",
    "                n_1 = grp_data[i].count()\n",
    "                n_2 = grp_data[j].count()\n",
    "                N = n_1 + n_2\n",
    "                mwu_eta_squared = ((U - (n_1*n_2/2)) / np.sqrt((n_1*n_2*(n_1+n_2+1))/12) / np.sqrt(n_1+n_2))**2\n",
    "                # according to http://www.statisticslectures.com/topics/mannwhitneyu/ & cross-checked with Origin & psychometrica\n",
    "                res_dict['U_mwu{}_vs_{}'.format(k,l)] = [U]\n",
    "                res_dict['eta^2_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared]\n",
    "                res_dict['p_mwu{}_vs_{}'.format(k,l)] = [mwu_p_value]\n",
    "                # Critical Value @0.05\n",
    "                p = 1-c_alpha if n_groups==2 else 1-(c_alpha/n_groups)\n",
    "                U_c = stats.chi2.ppf(p, 1)\n",
    "                mwu_eta_squared_c = U_c/N \n",
    "                # Eta squared can be calculated as η²=r²=chi²/N. Note that the Kruskal-Wallis H test statistic is approximately chi²-distributed.\n",
    "                res_dict['c_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared_c]\n",
    "                # Other Critical Valus\n",
    "                for c_al in c_alpha_list:\n",
    "                    p_al = 1-c_al if n_groups==2 else 1-(c_al/n_groups)\n",
    "                    U_c_al = stats.chi2.ppf(p_al, 1)\n",
    "                    mwu_eta_squared_c_al = U_c_al/N \n",
    "                    res_dict['c{}_mwu{}_vs_{}'.format(c_al, k,l)] = [mwu_eta_squared_c_al]\n",
    "                \n",
    "                # Direction check  \n",
    "                mwu_i_vs_j = 0  \n",
    "                if grp_medians[i] != grp_medians[j]:\n",
    "                    if all((n_groups==2, mwu_p_value <= c_alpha)) ^ all((n_groups>2, mwu_p_value <= c_alpha/n_groups)):\n",
    "                        mwu_i_vs_j = k if (grp_medians[i] > grp_medians[j]) else l\n",
    "                res_dict['mwu{}_vs_{}'.format(k,l)] = [mwu_i_vs_j] \n",
    "                \n",
    "                 \n",
    "        df_list += [pd.DataFrame(res_dict)]\n",
    "\n",
    "df_final = pd.concat(df_list)\n",
    "\n",
    "# Save to Figure source data\n",
    "df_final.to_csv('source_data/fig_{}/stat_results_quantifications_{}.csv'.format(FIG, LAB), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
