{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "if ROOT_DIR.endswith(\"notebooks\"):\n",
    "    # Go up one level to the repo root\n",
    "    os.chdir(os.path.dirname(ROOT_DIR))\n",
    "    \n",
    "# Import from unet library\n",
    "from unet import utils, smirnov_grubbs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Quantification and statistics for Figure 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'wue1'\n",
    "init = 'from-scratch'\n",
    "MIN_PIXEL = 30\n",
    "MASK = 'cFOS'\n",
    "AREA = 'NeuN'\n",
    "GROUPS = {'H' : 0, '-' : 1, '+' : 2}\n",
    "OUTLIERS_INT = [19]\n",
    "OUTLIERS = [1438]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Measure ROIs and area for all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Set paths and get model names\n",
    "data_path = 'bioimage_data/lab-{}/images'.format(LAB)\n",
    "mask_path = 'bioimage_data/lab-{}/labels'.format(LAB)\n",
    "area_path = 'bioimage_data/lab-{}/regions'.format(LAB)\n",
    "area_subfolder = ['CA1-CA3-DG_whole', 'CA1-CA3-DG_whole', 'CA1-CA3-DG_whole', 'DG_supra', 'DG_infra']\n",
    "areas = ['CA1', 'CA3', 'DG','DG', 'DG']\n",
    "subareas = ['CA1', 'CA3', 'DG','DG_supra', 'DG_infra']\n",
    "ens_list = [x for x in next(os.walk(os.path.join(mask_path,init)))[1] if not x.startswith('.')]\n",
    "assignment = pd.read_excel('bioimage_data/image_mapping.xlsx', converters={'ID': lambda x: str(x).zfill(4)})\n",
    "\n",
    "df_all = []\n",
    "for subfold, area, subarea in zip(area_subfolder, areas, subareas):\n",
    "    print(area, subarea)\n",
    "    \n",
    "    # Filter for regions\n",
    "    file_ids = assignment.loc[(assignment['name']=='lab-wue1') &\n",
    "                              (assignment['area'].isin([area])) & \n",
    "                              (assignment['training'].isna()), 'ID'].tolist()\n",
    "    \n",
    "    # Load images and areas\n",
    "    image_list = [io.imread(os.path.join(data_path, img_name), as_gray=True) for \n",
    "                  img_name in [s + '.tif' for s in file_ids]]\n",
    "    area_list = [io.imread(os.path.join(area_path, subfold, img_name), as_gray=True) for \n",
    "              img_name in [s + '_' + AREA + '.tif' for s in file_ids]]\n",
    "    \n",
    "    # Load and measure area\n",
    "    df_area_list = [utils.measure_rois(msk, img, fid) for msk, img, fid in zip(area_list, image_list, file_ids)]\n",
    "    df_area = pd.concat(df_area_list)\n",
    "    df_area.rename({'area': 'region_area'}, axis='columns', inplace=True)\n",
    "    \n",
    "    df_roi_list = []\n",
    "    for ens in tqdm_notebook(ens_list):\n",
    "        fold_list = [x for x in next(os.walk(os.path.join(mask_path, init, ens)))[1] if not x.startswith('.')]\n",
    "        model_list = [[x for x in next(os.walk(os.path.join(mask_path, init, ens, fold)))[1] if not x.startswith('.')][0] for fold in fold_list]\n",
    "        for fold, model in zip(fold_list, model_list):\n",
    "            model_path = os.path.join(mask_path, init, ens, fold, model)\n",
    "            \n",
    "            # Load masks\n",
    "            mask_list = [io.imread(os.path.join(model_path, x), as_gray=True).astype('int') for x in [s + '_' + MASK + '.png' for s in file_ids]]\n",
    "            \n",
    "            # Cut irrelevant regions\n",
    "            for msk, area in zip(mask_list, area_list):\n",
    "                msk[area==0]=0\n",
    "                \n",
    "            # Measure ROIs\n",
    "            df_list = [utils.measure_rois(msk, img, fid, min_pixel=MIN_PIXEL) for msk, img, fid in zip(mask_list, image_list, file_ids)]\n",
    "            \n",
    "            # Create output df\n",
    "            df_tmp = pd.concat(df_list)\n",
    "            df_tmp['model'] = fold\n",
    "            df_tmp['unet'] = model\n",
    "            df_tmp['ens'] = ens\n",
    "            df_roi_list += [df_tmp]\n",
    "            \n",
    "    df_roi = pd.concat(df_roi_list)    \n",
    "    df_roi['subarea'] = subarea\n",
    "    df_roi['init'] = init\n",
    "    df_roi = pd.merge(df_roi, df_area[['ID','region_area']], how='left', on='ID')\n",
    "    df_all += [df_roi]\n",
    "\n",
    "df_all = pd.concat(df_all)\n",
    "df_all.to_csv(os.path.join('source_data', 'fig_4', 'ROIs_'+LAB+'.csv'), index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Combine results with experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "assignment = pd.read_excel('bioimage_data/image_mapping.xlsx')\n",
    "df_zu = assignment[(assignment['name']=='lab-wue1')&\n",
    "                   (assignment['training'].isna())].copy()\n",
    "df_zu['Group'] = df_zu['condition'].transform(lambda x: GROUPS[x])\n",
    "\n",
    "# Merge Data\n",
    "df_all = pd.read_csv(os.path.join('source_data', 'fig_4', 'ROIs_'+LAB+'.csv'))\n",
    "df = (df_all.groupby(['subarea', 'ens', 'unet', 'model','init', 'ID'])\n",
    "      .agg({'mean_intensity' : ['mean', 'count'], 'region_area' : ['mean']}))\n",
    "df = df.join(df_zu.set_index('ID')[['condition', 'Group', 'experiment']], how='left', on='ID')\n",
    "df = df.rename(columns={('mean_intensity', 'count'): 'count', ('region_area', 'mean'): 'Neun_Area', ('mean_intensity', 'mean'): 'mean'})\n",
    "df['cfos_per_area'] = df['count']/df['Neun_Area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_out = df.copy()\n",
    "\n",
    "# Normalize data\n",
    "ctlr_norm = (df_out[df_out.Group == 0]\n",
    "             .groupby(['subarea', 'ens', 'unet', 'model', 'init', 'experiment'])[['cfos_per_area', 'mean']]\n",
    "             .agg(np.mean))\n",
    "df_out = (df_out.reset_index()\n",
    "          .set_index(['subarea', 'ens', 'unet', 'model','init','experiment', 'ID'])\n",
    "          .join(ctlr_norm, rsuffix='_ctrl'))\n",
    "df_out['norm_cfos_per_area'] = df_out['cfos_per_area']/df_out['cfos_per_area_ctrl']\n",
    "df_out['norm_mean_intensity'] = df_out['mean']/df_out['mean_ctrl']\n",
    "\n",
    "# Helper functions\n",
    "def get_outlier_names(x):\n",
    "    out_ind = smirnov_grubbs.two_sided_test_indices(x, alpha=0.05)\n",
    "    if len(out_ind)>0:\n",
    "        return str(x.iloc[out_ind].index.get_level_values('ID').values)\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "# Plot data\n",
    "df_plot = (df_out\n",
    "           .groupby(['subarea','unet', 'init','condition'])[['norm_cfos_per_area', 'norm_mean_intensity']]\n",
    "           .agg(get_outlier_names).reset_index())\n",
    "\n",
    "g = sns.catplot(x='condition', hue='norm_cfos_per_area', col='subarea',\n",
    "            data=df_plot, kind=\"count\",dodge=True, height=4, aspect=0.75)\n",
    "\n",
    "g = sns.catplot(x='condition', hue='norm_mean_intensity', col='subarea',\n",
    "            data=df_plot, kind=\"count\",dodge=True, height=4, aspect=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Remove Outliers ()\n",
    "df = df[~df.index.get_level_values('ID').isin(OUTLIERS)]\n",
    "df.loc[df.index.get_level_values('ID').isin(OUTLIERS_INT), 'mean'] = np.nan\n",
    "\n",
    "# Normalize data\n",
    "ctlr_norm = (df[df.Group == 0]\n",
    "             .groupby(['subarea', 'ens', 'unet', 'model', 'init', 'experiment'])[['cfos_per_area', 'mean']]\n",
    "             .agg(np.mean))\n",
    "df = (df.reset_index()\n",
    "          .set_index(['subarea', 'ens', 'unet', 'model','init','experiment', 'ID'])\n",
    "          .join(ctlr_norm, rsuffix='_ctrl'))\n",
    "df['norm_cfos_per_area'] = df['cfos_per_area']/df['cfos_per_area_ctrl']\n",
    "df['norm_mean_intensity'] = df['mean']/df['mean_ctrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Save to Figure source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Figure 4\n",
    "df.to_csv('source_data/fig_4/quantifications_wue1.csv')\n",
    "\n",
    "# Figure 3\n",
    "fig3_ens = ['wue1_consensus4_ensemble', 'wue1_consensus5_ensemble',\n",
    "            'wue1_consensus7_3.0007.h5', 'wue1_consensus1_3.0008.h5',\n",
    "            'wue1_expert4_3.0008.h5', 'wue1_expert2_4.0011.h5']\n",
    "df3 = df.reset_index()\n",
    "(df3[(df3.subarea=='CA1') & (df3.unet.isin(fig3_ens))]\n",
    " .to_csv('source_data/fig_3/quantifications_CA1_wue1.csv', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "c_alpha = 0.05\n",
    "c_alpha_list = [0.01, 0.001]\n",
    "for key2, grp2 in tqdm_notebook(df.groupby(['ens', 'unet', 'model','init', 'subarea'])):\n",
    "\n",
    "    for m in ['norm_cfos_per_area', 'norm_mean_intensity']:\n",
    "        res_dict = {'subarea': [key2[4]], 'ens': [key2[0]], \n",
    "                    'unet': [key2[1]], 'init': [key2[3]],\n",
    "                    'model': [key2[2]], 'type': [m]}\n",
    "        \n",
    "        df_fil = grp2[~grp2[m].isna()]\n",
    "        \n",
    "        if all(df_fil.groupby('Group').size()>3):\n",
    "            grp_data = [x[1] for x in df_fil.groupby('Group', sort=True)[m]]\n",
    "            n_groups = len(grp_data)\n",
    "            \n",
    "            # Get Group Means/Median\n",
    "            grp_means = [x.mean() for x in grp_data]   \n",
    "            res_dict['grp_mean_1'] = grp_means[0]\n",
    "            res_dict['grp_mean_2'] = grp_means[1]\n",
    "            res_dict['grp_mean_3'] = grp_means[2]\n",
    "            grp_medians = [x.median() for x in grp_data]       \n",
    "            \n",
    "            # Perform the Shapiro-Wilk test for normality.\n",
    "            grp_norms = [x[1] for x in map(stats.shapiro, grp_data)]\n",
    "            \n",
    "            #Check for equality of variances\n",
    "            if len(grp_data)>2:\n",
    "                _, levene = stats.levene(*grp_data, center = 'mean')\n",
    "            else:\n",
    "                _, levene = stats.levene(*grp_data, center = 'median')\n",
    "            \n",
    "            # Anova possible?\n",
    "            anova = True if all(np.array([*grp_norms,levene])> 0.05) else False\n",
    "            res_dict['anova_ok'] = [anova]\n",
    "            \n",
    "            # Only 3 groups tests\n",
    "            if n_groups==3: \n",
    "                \n",
    "                # kruskal all groups\n",
    "                H, kwa_p_value = stats.kruskal(*grp_data)\n",
    "                N = len(df_fil)\n",
    "                \n",
    "                kwa_eta_squared = (H - n_groups + 1)/(N-n_groups) # http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf\n",
    "                # kwa_eta_squared = ((H / (n_groups-1)) * (n_groups-1)) / ((H / (n_groups-1)) * (n_groups-1) + (N-n_groups)) \n",
    "                #For references, see: https://www.researchgate.net/post/Anyone_know_how_to_calculate_eta_squared_for_a_Kruskal-Wallis_analysis\n",
    "                res_dict['H_kwa_all'] = [H]\n",
    "                res_dict['eta^2_kwa_all'] = [kwa_eta_squared]\n",
    "                res_dict['p_kwa_all'] = [kwa_p_value]\n",
    "                res_dict['kwa_all'] = 1 if kwa_p_value<0.05 else 0\n",
    "                # Critical Value\n",
    "                p = 1-c_alpha\n",
    "                ddof = n_groups-1\n",
    "                H_c = stats.chi2.ppf(p, ddof) \n",
    "                kwa_eta_squared_c = (H_c- n_groups + 1)/(N-n_groups) \n",
    "                res_dict['c_kwa_all'] = [kwa_eta_squared_c]\n",
    "                \n",
    "            # Pairwise tests\n",
    "            for i,j in itertools.combinations(range(len(grp_data)), 2):\n",
    "                k, l = i+1, j+1\n",
    "                \n",
    "                # Pairwise mannwhitneyu tests\n",
    "                U, mwu_p_value = stats.mannwhitneyu(grp_data[i],grp_data[j], alternative = 'two-sided')\n",
    "                n_1 = grp_data[i].count()\n",
    "                n_2 = grp_data[j].count()\n",
    "                N = n_1 + n_2\n",
    "                mwu_eta_squared = ((U - (n_1*n_2/2)) / np.sqrt((n_1*n_2*(n_1+n_2+1))/12) / np.sqrt(n_1+n_2))**2\n",
    "                # according to http://www.statisticslectures.com/topics/mannwhitneyu/ & cross-checked with Origin & psychometrica\n",
    "                res_dict['U_mwu{}_vs_{}'.format(k,l)] = [U]\n",
    "                res_dict['eta^2_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared]\n",
    "                res_dict['p_mwu{}_vs_{}'.format(k,l)] = [mwu_p_value]\n",
    "                # Critical Value\n",
    "                p = 1-c_alpha if n_groups==2 else 1-(c_alpha/n_groups)\n",
    "                U_c = stats.chi2.ppf(p, 1)\n",
    "                mwu_eta_squared_c = U_c/N \n",
    "                # Eta squared can be calculated as η²=r²=chi²/N. Note that the Kruskal-Wallis H test statistic is approximately chi²-distributed.\n",
    "                res_dict['c_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared_c]\n",
    "                \n",
    "                # Other Critical Value\n",
    "                for c_al in c_alpha_list:\n",
    "                    p_al = 1-c_al if n_groups==2 else 1-(c_al/n_groups)\n",
    "                    U_c_al = stats.chi2.ppf(p_al, 1)\n",
    "                    mwu_eta_squared_c_al = U_c_al/N \n",
    "                    res_dict['c{}_mwu{}_vs_{}'.format(c_al, k,l)] = [mwu_eta_squared_c_al]\n",
    "                \n",
    "                # Direction check  \n",
    "                mwu_i_vs_j = 0  \n",
    "                if grp_medians[i] != grp_medians[j]:\n",
    "                    if all((n_groups==2, mwu_p_value <= c_alpha)) ^ all((n_groups>2, mwu_p_value <= c_alpha/n_groups)):\n",
    "                        mwu_i_vs_j = k if (grp_medians[i] > grp_medians[j]) else l\n",
    "                res_dict['mwu{}_vs_{}'.format(k,l)] = [mwu_i_vs_j] \n",
    "                 \n",
    "        df_list += [pd.DataFrame(res_dict)]\n",
    "\n",
    "df_final = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Save to Figure source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Figure 4\n",
    "df_final.to_csv('source_data/fig_4/stat_results_quantifications_{}.csv'.format(LAB), index=False)\n",
    "\n",
    "# Figure 3\n",
    "(df_final[(df_final.subarea=='CA1') & (df_final.unet.isin(fig3_ens)) & (df_final.type=='norm_cfos_per_area')]\n",
    " .to_csv('source_data/fig_3/stat_results_quantifications_{}.csv'.format(LAB), index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
