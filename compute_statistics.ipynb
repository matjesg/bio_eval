{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/data/DeepFLaSH/_bio_eval\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook\n",
    "from outliers import smirnov_grubbs as grubbs\n",
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "pd.set_option('max_columns', None)\n",
    "%cd _bio_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LAB = 'wue_all'\n",
    "groups = {'H' : 0, '-' : 1, '+' : 2}\n",
    "MASK = 'cFOS'\n",
    "OUTLIERS_INT = [19] #19 intensity\n",
    "OUTLIERS = [1438]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Run Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firstusr/anaconda3/envs/keras-gpu/lib/python3.7/site-packages/pandas/core/reshape/merge.py:618: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 1 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th>Experiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>1.243463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.988717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.149540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.122734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>1</th>\n",
       "      <td>1.166081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.018629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.117297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.119061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean\n",
       "Group Experiment          \n",
       "0     1           1.000000\n",
       "      2           1.000000\n",
       "      3           1.000000\n",
       "      4           1.000000\n",
       "1     1           1.243463\n",
       "      2           0.988717\n",
       "      3           1.149540\n",
       "      4           1.122734\n",
       "2     1           1.166081\n",
       "      2           1.018629\n",
       "      3           1.117297\n",
       "      4           1.119061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load Data\n",
    "df_zuordnung = pd.read_excel('../data/Zuordnung_corr.xlsx')\n",
    "df_zu = df_zuordnung[(df_zuordnung['Genotyp']=='WT')&\n",
    "                     (df_zuordnung['region']=='dHC')&\n",
    "                     (df_zuordnung['Area'].isin(['CA1', 'CA3', 'DG']))&\n",
    "                     (df_zuordnung['region']=='dHC')&\n",
    "                (df_zuordnung['Experiment'].isin([1,2,3,4]))&\n",
    "                (df_zuordnung['Cross-coder Training'].isna()) & \n",
    "                (df_zuordnung['Ausschluss von Analyse'].isna()) &\n",
    "                (df_zuordnung['broken'].isna())].copy()\n",
    "df_zu['Group'] = df_zu['Kondition'].transform(lambda x: groups[x])\n",
    "df_cfos = pd.read_csv(os.path.join(LAB,LAB+'_' + MASK + '_ROIs.csv'))\n",
    "\n",
    "#Merge Data\n",
    "df = df_cfos.groupby(['subarea', 'ens', 'unet', 'fold', 'Nummer']).agg({'mean_intensity' : ['mean', 'count'], 'Neun_Area' : ['mean']})\n",
    "df = df.join(df_zu.set_index('Nummer')[['Kondition', 'Group', 'Experiment']], how='left', on='Nummer')\n",
    "df = df.rename(columns={('mean_intensity', 'count'): 'count', ('Neun_Area', 'mean'): 'Neun_Area', ('mean_intensity', 'mean'): 'mean'})\n",
    "\n",
    "#Remove Outliers\n",
    "df = df[~df.index.get_level_values('Nummer').isin(OUTLIERS)]\n",
    "df.loc[df.index.get_level_values('Nummer').isin(OUTLIERS_INT), 'mean'] = np.nan\n",
    "\n",
    "#Normalize Data\n",
    "df['cfos_per_area'] = df['count']/df['Neun_Area']\n",
    "ctlr_norm = df[df.Group == 0].groupby(['subarea', 'ens', 'unet', 'fold', 'Experiment'])[['cfos_per_area', 'mean']].agg(np.mean)\n",
    "df = df.reset_index().set_index(['subarea', 'ens', 'unet', 'fold','Experiment', 'Nummer']).join(ctlr_norm, rsuffix='_ctrl')\n",
    "df['norm_cfos_per_area'] = df['cfos_per_area']/df['cfos_per_area_ctrl']\n",
    "df['norm_mean_intensity'] = df['mean']/df['mean_ctrl']\n",
    "\n",
    "display(df.groupby(['Group', 'Experiment']).norm_mean_intensity.agg(['mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "dfx = df.reset_index()\n",
    "dfx[(dfx['unet']=='ens_c')&(dfx.subarea=='CA3')].groupby(['Kondition']).norm_cfos_per_area.agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_outlier_names(x):\n",
    "    out_ind = grubbs.two_sided_test_indices(x, alpha=0.05)\n",
    "    if len(out_ind)>0:\n",
    "        return str(x.iloc[out_ind].index.get_level_values('Nummer').values)\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dfx = df.groupby(['subarea','ens', 'unet','Kondition'])[['norm_cfos_per_area', 'norm_mean_intensity']].agg(get_outlier_names).reset_index()\n",
    "CODER = ['ens_rohini','ens_manju','ens_dennis','ens_cora','ens_corinna']\n",
    "#dfx = dfx[~dfx.ens.isin(CODER)]\n",
    "#dfx = dfx[(~dfx.ens.isin(CODER))&(dfx.unet.str.startswith('ens'))]\n",
    "dfx[(dfx['norm_cfos_per_area']!='0')|(dfx['norm_mean_intensity']!='0')].to_csv('outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_out = df.groupby(['subarea','unet', 'Kondition'])[['norm_cfos_per_area', 'norm_mean_intensity']].agg(get_outlier_names).reset_index()\n",
    "g = sns.catplot(x='Kondition', hue='norm_mean_intensity', col='subarea',\n",
    "            data=df_out, kind=\"count\",dodge=True, height=4, aspect=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "g = sns.catplot(x='Kondition', hue='norm_cfos_per_area', col='subarea',\n",
    "            data=df_out, kind=\"count\",dodge=True, height=4, aspect=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Extra for CA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df.groupby('Nummer')[['norm_mean_intensity', 'norm_cfos_per_area']].agg('count').sort_values(by='norm_mean_intensity').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Compute effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4362f84e27c4dba8df5a95fe09d9129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "c_alpha = 0.05\n",
    "for key2, grp2 in tqdm_notebook(df.groupby(['ens', 'unet', 'fold', 'subarea'])):\n",
    "\n",
    "    for m in ['norm_cfos_per_area', 'norm_mean_intensity']:\n",
    "        res_dict = {'subarea': [key2[3]], 'ens': [key2[0]], 'unet': [key2[1]],\n",
    "                    'fold': [key2[2]], 'type': [m]}\n",
    "        \n",
    "        df_fil = grp2[~grp2[m].isna()]\n",
    "        \n",
    "        if all(df_fil.groupby('Group').size()>3):\n",
    "            grp_data = [x[1] for x in df_fil.groupby('Group', sort=True)[m]]\n",
    "            n_groups = len(grp_data)\n",
    "            \n",
    "            # Get Group Means/Median\n",
    "            grp_means = [x.mean() for x in grp_data]   \n",
    "            grp_medians = [x.median() for x in grp_data]       \n",
    "            \n",
    "            # Perform the Shapiro-Wilk test for normality.\n",
    "            grp_norms = [x[1] for x in map(stats.shapiro, grp_data)]\n",
    "            \n",
    "            #Check for equality of variances\n",
    "            if len(grp_data)>2:\n",
    "                _, levene = stats.levene(*grp_data, center = 'mean')\n",
    "            else:\n",
    "                _, levene = stats.levene(*grp_data, center = 'median')\n",
    "            \n",
    "            # Anova possible?\n",
    "            anova = True if all(np.array([*grp_norms,levene])> 0.05) else False\n",
    "            res_dict['anova_ok'] = [anova]\n",
    "            \n",
    "            # Only 3 groups tests\n",
    "            if n_groups==3: \n",
    "                \n",
    "                # kruskal all groups\n",
    "                H, kwa_p_value = stats.kruskal(*grp_data)\n",
    "                N = len(df_fil)\n",
    "                \n",
    "                kwa_eta_squared = (H - n_groups + 1)/(N-n_groups) # http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf\n",
    "                # kwa_eta_squared = ((H / (n_groups-1)) * (n_groups-1)) / ((H / (n_groups-1)) * (n_groups-1) + (N-n_groups)) \n",
    "                #For references, see: https://www.researchgate.net/post/Anyone_know_how_to_calculate_eta_squared_for_a_Kruskal-Wallis_analysis\n",
    "                res_dict['eta^2_kwa_all'] = [kwa_eta_squared]\n",
    "                res_dict['p_kwa_all'] = [kwa_p_value]\n",
    "                res_dict['kwa_all'] = 1 if kwa_p_value<0.05 else 0\n",
    "                # Critical Value\n",
    "                p = 1-c_alpha\n",
    "                ddof = n_groups-1\n",
    "                H_c = stats.chi2.ppf(p, ddof) \n",
    "                kwa_eta_squared_c = (H_c- n_groups + 1)/(N-n_groups) \n",
    "                res_dict['c_kwa_all'] = [kwa_eta_squared_c]\n",
    "                \n",
    "                # ANOVA All groups\n",
    "                aov = pg.anova(dv=m, between='Group', data=df_fil, detailed=False)\n",
    "                aov_p_value = aov['p-unc'][0]\n",
    "                res_dict['eta^2_aov_all'] = [aov['np2'][0]]\n",
    "                res_dict['p_aov_all'] = [aov_p_value]\n",
    "                res_dict['aov_all'] = 1 if aov_p_value<0.05 else 0\n",
    "                # Critical Value\n",
    "                # Calculating partial eta-square (fval * ddof1) / (fval * ddof1 + ddof2) \n",
    "                # https://pingouin-stats.org/_modules/pingouin/parametric.html#anova\n",
    "                ddof1 = n_groups - 1\n",
    "                ddof2 = N - n_groups\n",
    "                fval = stats.f.ppf(p, ddof1, ddof2) \n",
    "                aov_eta_squared_c = (fval * ddof1) / (fval * ddof1 + ddof2) \n",
    "                res_dict['c_aov_all'] = [aov_eta_squared_c]\n",
    "            # Pairwise tests\n",
    "            for i,j in itertools.combinations(range(len(grp_data)), 2):\n",
    "                k, l = i+1, j+1\n",
    "                \n",
    "                # Pairwise mannwhitneyu tests\n",
    "                U, mwu_p_value = stats.mannwhitneyu(grp_data[i],grp_data[j], alternative = 'two-sided')\n",
    "                n_1 = grp_data[i].count()\n",
    "                n_2 = grp_data[j].count()\n",
    "                N = n_1 + n_2\n",
    "                mwu_eta_squared = ((U - (n_1*n_2/2)) / np.sqrt((n_1*n_2*(n_1+n_2+1))/12) / np.sqrt(n_1+n_2))**2\n",
    "                # according to http://www.statisticslectures.com/topics/mannwhitneyu/ & cross-checked with Origin & psychometrica\n",
    "                res_dict['eta^2_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared]\n",
    "                res_dict['p_mwu{}_vs_{}'.format(k,l)] = [mwu_p_value]\n",
    "                # Critical Value\n",
    "                p = 1-c_alpha if n_groups==2 else 1-(c_alpha/n_groups)\n",
    "                U_c = stats.chi2.ppf(p, 1)\n",
    "                mwu_eta_squared_c = U_c/N \n",
    "                # Eta squared can be calculated as η²=r²=chi²/N. Note that the Kruskal-Wallis H test statistic is approximately chi²-distributed.\n",
    "                res_dict['c_mwu{}_vs_{}'.format(k,l)] = [mwu_eta_squared_c]\n",
    "                # Direction check  \n",
    "                mwu_i_vs_j = 0  \n",
    "                if grp_medians[i] != grp_medians[j]:\n",
    "                    if all((n_groups==2, mwu_p_value <= c_alpha)) ^ all((n_groups>2, mwu_p_value <= c_alpha/n_groups)):\n",
    "                        mwu_i_vs_j = k if (grp_medians[i] > grp_medians[j]) else l\n",
    "                res_dict['mwu{}_vs_{}'.format(k,l)] = [mwu_i_vs_j] \n",
    "                \n",
    "                # Pairwise ANOVA\n",
    "                aov = pg.anova(dv=m, between='Group', data=df_fil[df_fil['Group'].isin([i,j])], detailed=False)\n",
    "                aov_p_value = aov['p-unc'][0]   \n",
    "                res_dict['eta^2_aov{}_vs_{}'.format(k,l)] = [aov['np2'][0]]\n",
    "                res_dict['p_aov{}_vs_{}'.format(k,l)] = [aov_p_value]\n",
    "                # Critical Value\n",
    "                ddof1 = 2-1\n",
    "                ddof2 = N-2\n",
    "                fval = stats.f.ppf(p, ddof1, ddof2) \n",
    "                aov_eta_squared_c = (fval * ddof1) / (fval * ddof1 + ddof2) \n",
    "                res_dict['c_aov{}_vs_{}'.format(k,l)] = [aov_eta_squared_c]\n",
    "                # Direction check\n",
    "                aov_i_vs_j = 0  \n",
    "                if grp_means[i] != grp_means[j]:\n",
    "                    if all((n_groups==2, aov_p_value <= c_alpha)) ^ all((n_groups>2, aov_p_value <= c_alpha/n_groups)):\n",
    "                        aov_i_vs_j = k if (grp_means[i] > grp_means[j]) else l\n",
    "                res_dict['aov{}_vs_{}'.format(k,l)] = [aov_i_vs_j] \n",
    "                #assert (all((key2[3]=='DG',[key2[1]]==['ens_7'], 'aov{}_vs_{}'.format(k,l)=='aov2_vs_3')))!=True\n",
    "                 \n",
    "        df_list += [pd.DataFrame(res_dict)]\n",
    "\n",
    "df_final = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_final.to_csv(os.path.join('_results/'+ LAB +'_effect.csv'), index=False)\n",
    "df_final[df_final.type=='norm_cfos_per_area'].to_csv(os.path.join('_results/'+ LAB+'_effect_count.csv'), index=False)\n",
    "df_final[df_final.type=='norm_mean_intensity'].to_csv(os.path.join('_results/'+ LAB+'_effect_intensity.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
